---
- name: Create Infrastructure
  hosts: localhost
  tasks:
    #AWS customer infrastructure role called, this creates the required infrastructure for a customer compartment
    - name: Create VPC, Subnets, NAT Gateways, Route Tables, Security Groups and Route53 Zones
      include_role:
        name: aws-customer-infrastructure
      vars:
        #A list of CIDR ranges for the VPC is passed in
        aws_customer_vpc_cidr: "{{ vpc_cidr }}"
        #The desired name for the VPC is given
        aws_customer_vpc_env_name: "{{ vpc_env_name }}"
        #The desired region is given
        aws_customer_vpc_region: "{{ vpc_region }}"
        #The CIDR range for each Public subnet is given
        publicaz1_cidr: "{{ pubaz1_cidr }}"
        publicaz2_cidr: "{{ pubaz2_cidr }}"
        publicaz3_cidr: "{{ pubaz3_cidr }}"
        #The CIDR range for each Management subnet is given
        managementaz1_cidr: "{{ manaz1_cidr }}"
        managementaz2_cidr: "{{ manaz2_cidr }}"
        managementaz3_cidr: "{{ manaz3_cidr }}"
        #The CIDR range for each Customer subnet is given
        customeraz1_cidr: "{{ custaz1_cidr }}"
        customeraz2_cidr: "{{ custaz2_cidr }}"
        customeraz3_cidr: "{{ custaz3_cidr }}"
        #The CIDR range for each Clustering/Reserved subnet is given
        clusteraz1_cidr: "{{ clustaz1_cidr }}"
        clusteraz2_cidr: "{{ clustaz2_cidr }}"
        clusteraz3_cidr: "{{ clustaz3_cidr }}"
        #The CIDR range of the attached management compartment is given
        aws_customer_vpc_management_cidr: "{{ vpc_management_cidr }}"
        #The management dns zone is given as shortname
        aws_customer_vpc_shortname: "{{ vpc_shortname }}"
        #The customer dns zone is given as longname
        aws_customer_vpc_longname: "{{ vpc_longname }}"
        #create_zone is set to true as this is the primary compartment and DNS zones need to be created
        create_zone: true
        #The single_az flag is used to indicate if additional AZ subnets need to created
        single_az: "{{ single_availability_zone }}"
        dr: false
#Provisioning  SIEM server
- name: Provision SIEM server
  hosts: localhost
  tasks:
    - name: include siem role
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: SIEM
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ siem_instance_type }}"
        aws_ec2_vm_image: "{{ siem_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ siem_eth0_security_groups }}"
        single_nic: true
        instance_profile_role: "{{ siem_instance_profile_name }}"
      when: siem
    #An entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "siem-srv.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: siem
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "siem-srv.{{ vpc_shortname }}"
        wait: yes
      when: siem
    - name: Add SIEM to siem group
      add_host:
        name: "{{ eth0.interface.private_ip_address }}"
        groups: siem
      when: siem


- name: Provision Component Machines
  hosts: localhost
  tasks:
    #Provision FDE VM, this machine has two network interfaces.
    - name: Provision FDE ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        #Vars are passed in via the group_vars file, single_nic flag is set to false as this machine needs a customer and a management interface
        aws_ec2_vm_Name: FDE
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ fde_instance_type }}"
        aws_ec2_vm_image: "{{ fde_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz1.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ cust_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ customeraz1.subnet.id }}"
        aws_eth0_security_groups: "{{ fde_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ fde_eth1_security_groups }}"
        single_nic: false
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 30
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 50
    #The created instance is added to the FDE hosts group, so it can later be configured.    
    - name: Add FDE to fde group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: fde
    #A management DNS entry is added to the management zone    
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "fde.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth1.interface.private_ip_address }}"
        wait: yes
    #A customer DNS entry is added to the customer zone
    - route53:
        state: present
        zone: "{{ vpc_longname }}"
        record: "fde.{{ vpc_longname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
    #Reverse DNS entries are added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth1.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "fde.{{ vpc_shortname }}"
        wait: yes
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "fde.{{ vpc_longname }}"
        wait: yes

    #FDE 2 VM is created if the HA flag is set to true, and is created in AZ2        
    - name: Provision FDE-2 ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: FDE-2
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ fde_instance_type }}"
        aws_ec2_vm_image: "{{ fde_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz2.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ cust_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ customeraz2.subnet.id }}"
        aws_eth0_security_groups: "{{ fde_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ fde_eth1_security_groups }}"
        single_nic: false
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 30
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 50
      when: ha
    #The created instance is added to the FDE hosts group, so it can later be configured.
    - name: Add FDE to fde-2 group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: fde
      when: ha
    
    #MidServerA VM is created, only a management interface is created   
    - name: Provision MidServerA ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: MidServerA
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ mid_instance_type }}"
        aws_ec2_vm_image: "{{ mid_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ mid_eth0_security_groups }}"
        single_nic: true
    #A management entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "mida.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "mida.{{ vpc_shortname }}"
        wait: yes
    #The created mid server instance is added to the mids group, so it can later be configured.    
    - name: Add mid1 to mids group
      add_host:
        name: "{{ eth0.interface.private_ip_address }}"
        groups: mids
        
    #MidServerB VM is created, only a management interface is created
    - name: Provision MidServerB ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: MidServerB
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ mid_instance_type }}"
        aws_ec2_vm_image: "{{ mid_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ mid_eth0_security_groups }}"
        single_nic: true
    #A management entry is added to the management DNS zon
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "midb.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "midb.{{ vpc_shortname }}"
        wait: yes
    #The created mid server instance is added to the mids group, so it can later be configured.
    - name: Add mid2 to mids group
      add_host:
        name: "{{ eth0.interface.private_ip_address }}"
        groups: mids

    #The BUaaS-Networker-Server VM is created, if the BUaaS flag is set to true
    - name: Provision BUaaS-Networker-Server
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: BUaaS-Networker-Server
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ buaas_nwr_srv_instance_type }}"
        aws_ec2_vm_image: "{{ buaas_nwr_srv_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ buaas_eth0_security_groups }}"
        single_nic: true
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 500
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 500
      when: buaas
    #An entry is added to the management DNS zone    
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "nwr-srv.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: buaas
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "nwr-srv.{{ vpc_shortname }}"
        wait: yes
      when: buaas
    #The BUaaS-Networker-Console VM is created, if the BUaaS flag is set to true
    - name: Provision BUaaS-Networker-Console
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: BUaaS-Networker-Console
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ buaas_nwr_con_instance_type }}"
        aws_ec2_vm_image: "{{ buaas_nwr_con_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ buaas_eth0_security_groups }}"
        single_nic: true
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 40
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 40
      when: buaas
    #An entry is added to the management DNS zone    
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "nwr-con.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: buaas
    #A reverse entry is added to the reverse zone    
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "nwr-con.{{ vpc_shortname }}"
        wait: yes
      when: buaas
    #The BUaaS-Cloudboost-server VM is created, if the BUaaS flag is set to true
    - name: Provision BUaaS-Cloudboost-Server
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: BUaaS-Cloudboost-Server
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ buaas_cb_instance_type }}"
        aws_ec2_vm_image: "{{ buaas_cb_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ buaas_eth0_security_groups }}"
        single_nic: true
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 50
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 500
      when: buaas
    #An entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "nwr-cb.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: buaas
    #A reverse entry is added to the reverse zone    
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "nwr-cb.{{ vpc_shortname }}"
        wait: yes
      when: buaas
    #SAP Router VM is created
    - name: Provision SAP Router ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: SAPRouter
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ proxy_instance_type }}"
        aws_ec2_vm_image: "{{ proxy_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz1.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ pub_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ publicaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ sap_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ sap_eth1_security_groups }}"
        single_nic: false
    #The created sap router instance is added to the saprouter group, so it can later be configured.    
    - name: Add saprouter to saprouter group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: saprouter
    #An entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "proxy.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth1.interface.private_ip_address }}"
        wait: yes
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth1.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "proxy.{{ vpc_shortname }}"
        wait: yes


    #SAP Router 2 VM is created, if the HA flag is set to true    
    - name: Provision SAP-2 Router ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: SAPRouter-2
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ proxy_instance_type }}"
        aws_ec2_vm_image: "{{ proxy_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz2.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ pub_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ publicaz2.subnet.id }}"
        aws_eth0_security_groups: "{{ sap_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ sap_eth1_security_groups }}"
        single_nic: false
      when: ha
    #Created SAP Router instance is added to sap router group, so it can later be configured.  
    - name: Add saprouter2 to saprouter group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: saprouter
      when: ha


- name: Create DR Infrastructure
  hosts: localhost
  tasks:
    #If DR option is selected, a second VPC and associated infrastructure is created for DR, create_zone is set to false, as the DNS zones are created with the primary VPC
    - name: Create VPC, Subnets, NAT Gateways, Route Tables, Security Groups and Route53 Zones
      include_role:
        name: aws-customer-infrastructure
      vars:
        aws_customer_vpc_cidr: "{{ dr_vpc_cidr }}"
        aws_customer_vpc_env_name: "{{ dr_vpc_env_name }}"
        aws_customer_vpc_region: "{{ dr_vpc_region }}"
        publicaz1_cidr: "{{ dr_pubaz1_cidr }}"
        publicaz2_cidr: "{{ dr_pubaz2_cidr }}"
        publicaz3_cidr: "{{ dr_pubaz3_cidr }}"
        managementaz1_cidr: "{{ dr_manaz1_cidr }}"
        managementaz2_cidr: "{{ dr_manaz2_cidr }}"
        managementaz3_cidr: "{{ dr_manaz3_cidr }}"
        customeraz1_cidr: "{{ dr_custaz1_cidr }}"
        customeraz2_cidr: "{{ dr_custaz2_cidr }}"
        customeraz3_cidr: "{{ dr_custaz3_cidr }}"
        clusteraz1_cidr: "{{ dr_clustaz1_cidr }}"
        clusteraz2_cidr: "{{ dr_clustaz2_cidr }}"
        clusteraz3_cidr: "{{ dr_clustaz3_cidr }}"
        aws_customer_vpc_management_cidr: "{{ vpc_management_cidr }}"
        aws_customer_vpc_shortname: "{{ vpc_shortname }}"
        aws_customer_vpc_longname: "{{ vpc_longname }}"
        create_zone: false
        single_az: "{{ dr_single_availability_zone }}"
        dr: true
      when: dr_compartment

- name: Provision Component Machines
  hosts: localhost
  tasks:
    #Provision FDE VM, this machine has two network interfaces
    - name: Provision FDE ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: FDE-DR
        aws_ec2_vm_region: "{{ dr_vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ fde_instance_type }}"
        aws_ec2_vm_image: "{{ dr_fde_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz1.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ cust_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ customeraz1.subnet.id }}"
        aws_eth0_security_groups: "{{ fde_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ fde_eth1_security_groups }}"
        single_nic: false
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 30
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 50
      when: dr_compartment
    #The created instance is added to the FDE hosts group, so it can later be configured.  
    - name: Add dr fde to fde group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: fde
      when: dr_compartment
    #A management DNS entry is added to the management zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "fdedr.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth1.interface.private_ip_address }}"
        wait: yes
      when: dr_compartment
    #A customer DNS entry is added to the customer zone
    - route53:
        state: present
        zone: "{{ vpc_longname }}"
        record: "fdedr.{{ vpc_longname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: dr_compartment
    #Reverse DNS entries are added to the reverse zone    
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ rev_zone }}"
        record: "{{ eth1.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "fdedr.{{ vpc_shortname }}"
        wait: yes
      when: dr_compartment
    #Reverse DNS entries are added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ rev_zone }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "fdedr.{{ vpc_longname }}"
        wait: yes
      when: dr_compartment
    #FDE 2 VM is created if the HA flag is set to true, and is created in AZ2
    - name: Provision FDE-2 ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: FDE-DR-2
        aws_ec2_vm_region: "{{ dr_vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ fde_instance_type }}"
        aws_ec2_vm_image: "{{ dr_fde_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz2.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ cust_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ customeraz2.subnet.id }}"
        aws_eth0_security_groups: "{{ fde_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ fde_eth1_security_groups }}"
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 30
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 50
        single_nic: false
      when: dr_ha
    #The created instance is added to the FDE hosts group, so it can later be configured.  
    - name: Add dr fde2 to fde group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: fde
      when: dr_compartment and dr_ha
    #MidServerA VM is created, only a management interface is created  
    - name: Provision MidServerA ec2 VM DR
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: MidServerA-DR
        aws_ec2_vm_region: "{{ dr_vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ mid_instance_type }}"
        aws_ec2_vm_image: "{{ dr_mid_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ mid_eth0_security_groups }}"
        single_nic: true
      when: dr_compartment
    #A management entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "mida-dr.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: dr_compartment
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ rev_zone }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "mida-dr.{{ vpc_shortname }}"
        wait: yes
      when: dr_compartment
    #The created mid server instance is added to the mids group, so it can later be configured.  
    - name: Add dr mid1 to mids group
      add_host:
        name: "{{ eth0.interface.private_ip_address }}"
        groups: mids
      when: dr_compartment
    #MidServerB VM is created, only a management interface is created    
    - name: Provision MidServerB ec2 VM DR
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: MidServerB-DR
        aws_ec2_vm_region: "{{ dr_vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ mid_instance_type }}"
        aws_ec2_vm_image: "{{ dr_mid_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ mid_eth0_security_groups }}"
        single_nic: true
      when: dr_compartment
    #A management entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "midb-dr.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: dr_compartment
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ rev_zone }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "midb-dr.{{ vpc_shortname }}"
        wait: yes
      when: dr_compartment
    #The created mid server instance is added to the mids group, so it can later be configured.  
    - name: Add dr mid2 to mids group
      add_host:
        name: "{{ eth0.interface.private_ip_address }}"
        groups: mids
      when: dr_compartment
   
    #SAP Router VM is created
    - name: Provision SAP Router ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: SAPRouter-DR
        aws_ec2_vm_region: "{{ dr_vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ proxy_instance_type }}"
        aws_ec2_vm_image: "{{ dr_proxy_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz1.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ pub_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ publicaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ sap_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ sap_eth1_security_groups }}"
        single_nic: false
      when: dr_compartment
    #The created sap router instance is added to the saprouter group, so it can later be configured.  
    - name: Add dr saprouter to saprouter group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: saprouter
      when: dr_compartment
    #An entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "proxydr.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth1.interface.private_ip_address }}"
        wait: yes
      when: dr_compartment
    #A reverse entry is added to the reverse zone    
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ rev_zone }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "proxydr.{{ vpc_longname }}"
        wait: yes
      when: dr_compartment
    #SAP Router 2 VM is created, if HA flag is set to true  
    - name: Provision SAP Router ec2 VM
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: SAPRouter-DR
        aws_ec2_vm_region: "{{ dr_vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ proxy_instance_type }}"
        aws_ec2_vm_image: "{{ dr_proxy_image }}"
        aws_ec2_vm_eth1_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth1_subnet: "{{ managementaz2.subnet.id }}"
        aws_ec2_vm_eth0_groups: "{{ pub_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ publicaz2.subnet.id }}"
        aws_eth0_security_groups: "{{ sap_eth0_security_groups }}"
        aws_eth1_security_groups: "{{ sap_eth1_security_groups }}"
        single_nic: false
      when: dr_ha
    #The created sap router instance is added to the saprouter group, so it can later be configured.  
    - name: Add dr saprouter2 to saprouter group
      add_host:
        name: "{{ eth1.interface.private_ip_address }}"
        groups: saprouter
      when: dr_compartment and dr_ha

    #Primary and DR VPCs are peered using peer role   
    - name: Peer VPCs
      include_role:
        name: peer
      vars:
        region1: "{{ vpc_region }}"
        vpc1: "{{ vpcid }}"
        region2: "{{ dr_vpc_region }}"
        vpc2: "{{ drvpcid }}"
        peer_name: "DR Peering"
      when: dr_compartment
    #Primary Customer Route Table is updated to provide route to DR
    - name: Update Route Table Customer
      ec2_vpc_route_table:
        vpc_id: "{{ vpcid }}"
        region: "{{ vpc_region }}"
        route_table_id: "{{ custrt }}"
        tags:
          Name: Customer
        routes:
          - dest: "{{ dr_manaz1_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
      when: dr_compartment
    #Primary Management Route Table AZ1 is updated to provide route to DR
    - name: Update Route Table Management AZ1
      ec2_vpc_route_table:
        vpc_id: "{{ vpcid }}"
        region: "{{ vpc_region }}"
        route_table_id: "{{ manaz1rt }}"
        tags:
          Name: Management AZ1
        routes:
          - dest: "{{ dr_manaz1_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
          - dest: 0.0.0.0/0
            gateway_id: "{{ az1ngw }}"
      when: dr_compartment
    #Primary Management Route Table AZ2 is updated to provide route to DR  
    - name: Update Route Table Management AZ2
      ec2_vpc_route_table:
        vpc_id: "{{ vpcid }}"
        region: "{{ vpc_region }}"
        route_table_id: "{{ manaz2rt }}"
        tags:
          Name: Management AZ2
        routes:
          - dest: "{{ dr_manaz2_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
          - dest: 0.0.0.0/0
            gateway_id: "{{ az2ngw }}"
      when: dr_compartment and not single_availability_zone
    #Primary Management Route Table AZ3 is updated to provide route to DR  
    - name: Update Route Table Management AZ3
      ec2_vpc_route_table:
        vpc_id: "{{ vpcid }}"
        region: "{{ vpc_region }}"
        route_table_id: "{{ manaz3rt }}"
        tags:
          Name: Management AZ3
        routes:
          - dest: "{{ dr_manaz3_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
          - dest: 0.0.0.0/0
            gateway_id: "{{ az3ngw }}"
      when: dr_compartment and not single_availability_zone
    #Customer Route Table is updated to provide route to Primary  
    - name: Update DR Route Table Customer
      ec2_vpc_route_table:
        vpc_id: "{{ drvpcid }}"
        region: "{{ dr_vpc_region }}"
        route_table_id: "{{ drcustrt }}"
        tags:
          Name: Customer
        routes:
          - dest: "{{ manaz1_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
      when: dr_compartment
    #DR Management Route Table AZ1 is updated to provide route to Primary  
    - name: Update DR Route Table Management AZ1
      ec2_vpc_route_table:
        vpc_id: "{{ drvpcid }}"
        region: "{{ dr_vpc_region }}"
        route_table_id: "{{ drmanaz1rt }}"
        tags:
          Name: Management AZ1
        routes:
          - dest: "{{ manaz1_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
          - dest: 0.0.0.0/0
            gateway_id: "{{ draz1ngw }}"
      when: dr_compartment
    #DR Management Route Table AZ2 is updated to provide route to Primary  
    - name: Update DR Route Table Management AZ2
      ec2_vpc_route_table:
        vpc_id: "{{ drvpcid }}"
        region: "{{ dr_vpc_region }}"
        route_table_id: "{{ drmanaz2rt }}"
        tags:
          Name: Management AZ2
        routes:
          - dest: "{{ manaz2_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
          - dest: 0.0.0.0/0
            gateway_id: "{{ draz2ngw }}"
      when: dr_compartment and not dr_single_availability_zone
    #DR Management Route Table AZ3 is updated to provide route to Primary  
    - name: Update DR Route Table Management AZ3
      ec2_vpc_route_table:
        vpc_id: "{{ drvpcid }}"
        region: "{{ dr_vpc_region }}"
        route_table_id: "{{ drmanaz3rt }}"
        tags:
          Name: Management AZ3
        routes:
          - dest: "{{ manaz3_cidr }}"
            vpc_peering_connection_id: "{{ vpc_peer.peering_id }}"
          - dest: 0.0.0.0/0
            gateway_id: "{{ draz3ngw }}"
      when: dr_compartment and not dr_single_availability_zone
    #Update Security group in DR to add rule allowing primary CIDR range in on all ports  
    - ec2_group:
        name: mgmt-net-all
        description: Every AWS interface in the VPCs AZ1 2 3 management networks must be assigned to this security group
        vpc_id: "{{ drvpcid }}"
        region: "{{ dr_vpc_region }}"
        rules:
          - proto: -1
            ports: 0
            group_name: mgmt-net-all
          - proto: -1
            ports: 0
            cidr_ip: "{{ vpc_management_cidr  }}"
          - proto: -1
            ports: 0
            cidr_ip: "{{ manaz1_cidr.split('.')[0:2] | join('.') }}.0.0/12"
      register: mgmt_net_all_dr
      when: dr_compartment

    #The SUMA-Server VM is created, if the SUMA flag is set to true
    - name: Provision SUMA-Server
      include_role:
        name: aws-ec2-vm
      vars:
        aws_ec2_vm_Name: SUMA-Server
        aws_ec2_vm_region: "{{ vpc_region }}"
        aws_ec2_vm_key_name: "{{ key_name }}"
        aws_ec2_vm_instance_type: "{{ suma_instance_type }}"
        aws_ec2_vm_image: "{{ suma_image }}"
        aws_ec2_vm_eth0_groups: "{{ mgmt_net_all.group_id }}"
        aws_ec2_vm_eth0_subnet: "{{ managementaz1.subnet.id }}"
        aws_eth0_security_groups: "{{ suma_eth0_security_groups }}"
        single_nic: true
        aws_ec2_vm_volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            volume_size: 150
          - device_name: /dev/sdb
            volume_type: gp2
            volume_size: 50
          - device_name: /dev/sdc
            volume_type: gp2
            volume_size: 500
      when: suma
    #An entry is added to the management DNS zone
    - route53:
        state: present
        zone: "{{ vpc_shortname }}"
        record: "suma-srv.{{ vpc_shortname }}"
        type: A
        private_zone: yes
        ttl: 300
        value: "{{ eth0.interface.private_ip_address }}"
        wait: yes
      when: suma
    #A reverse entry is added to the reverse zone
    - route53:
        state: present
        zone: in-addr.arpa
        hosted_zone_id: "{{ reverse_zone.zone_id }}"
        record: "{{ eth0.interface.private_ip_address | ipaddr('revdns') }}"
        type: PTR
        private_zone: yes
        ttl: 300
        value: "suma-srv.{{ vpc_shortname }}"
        wait: yes
      when: suma
      #The created instance is added to the SUMA hosts group, so it can later be configured.
    - name: Add SUMA server to suma group
      add_host:
        name: "{{ eth0.interface.private_ip_address }}"
        groups: suma
      when: suma


#CSR Spoke creation role ran to connect new customer compartment to desired management compartment      
- name: CSR Spoke Creation
  hosts: localhost
  tasks:
    - include_role:
        name: csr_automation

- name: midserver
  hosts: mids
  tasks:
    - name: cli installation of midserver
      include_role:
        name: midserver
    - name: Set authorized key taken from file
      authorized_key:
        user: ec2-user
        state: present
        key: ""

#SAP Router configuration ran on SAP Router hosts
- name: SAP Router Configuration
  hosts: saprouter
  tasks:
    - name: Call SAP Router Role
      include_role:
        name: saprouter
      vars:
        management_subnet_cidr: "{{ vpc_management_cidr }}"
        #This is the Customer Management LAN - AZ1
        acl_localnet_src_a: "{{ manaz1_cidr if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr }}"
        #This is the Customer Management LAN - AZ2
        acl_localnet_src_b: "{{ manaz2_cidr if ansible_default_ipv4.address | match('172.') else dr_manaz2_cidr }}"
        static_route_ip: "{{ manaz1_cidr.split('.')[0:3] | join('.') if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr.split('.')[0:3] | join('.') }}.1"
        #This is the command for the desired IP route
        static_route: route add default gw "{{ static_route_ip }}" eth1
        azure_region: West US

- name: configure siem server
  hosts: siem
  become: yes
  tasks:
    - name: configuration of siem server
      include_role:
        name: siem
        tasks_from: configure-siem-server.yml

#FDE configuration ran on FDE hosts
- name: Configure FDE Server
  hosts: fde
  tasks:
    - name: Call role
      include_role:
        name: fde
      vars:
        #The customer and management subnet CIDR ranges should cover multiple AZ's if deploying on AWS, for example 172.17.104.0/21 would cover multiple AZs
        customer_subnet_cidr: "{{ custaz1_cidr.split('/')[0] if ansible_default_ipv4.address | match('172.') else dr_custaz1_cidr.split('/')[0] }}/21"
        customer_management_subnet_cidr: "{{ manaz1_cidr.split('/')[0] if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr.split('/')[0] }}/21"
        #This is the IP of the LaMa instance for the associated management compartment
        lama_ip: "{{ vpc_management_cidr.split('.')[0:3] | join('.') }}.41"
        #Customer Lan (all AZ's) of customer
        customer_lan: "{{ '172.16.0.0/12' if ansible_default_ipv4.address | match('172.') else '100.64.0.0/12' }}"
        #Customer Lan (all AZ's) of customer
        management_lan: "{{ '172.17.0.0/12' if ansible_default_ipv4.address | match('172.') else '100.65.0.0/12' }}"
        #IPs of Dev/Test DNS Forwarders
        dns_ip_dev: "10.4.0.5"
        dns_ip_test: "10.2.0.5"
        #DNS Zones for mangement, customer and reverse DNS
        management_zone: "{{ vpc_shortname }}"
        customer_zone: "{{ vpc_longname }}"
        reverse_zone: "in-addr.arpa"
        #Forwarder IPs for each zone, for Azure these are taken from the nameserver addresses for each zone. For AWS it is by default the x.x.x.2 of the CIDR, e.g. 172.17.104.2
        management_forwarders: "{{ manaz1_cidr.split('.')[0:3] | join('.') if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr.split('.')[0:3] | join('.') }}.2;"
        customer_forwarders: "{{ manaz1_cidr.split('.')[0:3] | join('.') if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr.split('.')[0:3] | join('.') }}.2;"
        reverse_forwarders: "{{ manaz1_cidr.split('.')[0:3] | join('.') if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr.split('.')[0:3] | join('.') }}.2;"
        #Forwarder IP, if Azure then 168.63.129.16, if AWS then first IP in management range, e.g. 172.17.0.2
        forwarder_ip: "{{ manaz1_cidr.split('.')[0:3] | join('.') if ansible_default_ipv4.address | match('172.') else dr_manaz1_cidr.split('.')[0:3] | join('.') }}.2;"
        #FDE customer domain is the customer domain
        fde_customer_domain: "{{ vpc_longname }}"
        #Management IP of the FDE itself
        fde_ip: "{{ ansible_default_ipv4.address }}"
        #Set saprouter proxy address
        proxy_domain: "{{ 'proxy' if ansible_default_ipv4.address | match('172.') else 'proxydr' }}.{{ vpc_shortname }}"
        fde_virualhostname: "{{ 'fde' if ansible_default_ipv4.address | match('172.') else 'fdedr' }}.{{ vpc_shortname }}"
      when: configure_fde

#SUMA configuration ran on SUMA host
- name: SUMA Server Configuration
  hosts: suma
  tasks:
    - name: Call SUMA Server Role
      include_role:
        name: SUMA
      vars:
        #This is the registration code for SUSEConnect found in SCC
        SUSEConnect_reg_code: 5490FA1496BF2C
        #This is the email address for main account (System-Team)
        Email_address: e4s-publiccloud_system@dxc.com
        #This is the registration code for SUSE Manager Service found in SCC
        SUSEManager_reg_code: 692572E0C0DAA8
        fstab: /dev/nvme2n1p1 /manager_storage xfs defaults,nofail 0 1
        FQDN: '{{ suma_short }}.{{ vpc_shortname }}'
        #IP address of box
        IP: "{{ ansible_default_ipv4.address }}"
        Short_name: '{{ suma_short }}'
